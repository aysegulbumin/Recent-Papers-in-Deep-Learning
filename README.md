# Recent-Papers-in-Deep-Learning
This repository is to share the recent papers we read and discuss as a friend group.
We will try to cover broad topics, following the top-tier conferences as well as journals.

## The Topics:

#### Reinforcement Learning:

- [No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium](https://arxiv.org/abs/2004.00603)

#### Optimization:

- [AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients](https://arxiv.org/abs/2010.07468)

#### Useful Blogs:

- [Lipschitz Continuity](https://xingyuzhou.org/blog/notes/Lipschitz-gradient)

## Transformer 
#### (presented by subhash - 01/28/2021)
Resources:
* [Attention is all you need paper](https://arxiv.org/pdf/1706.03762.pdf)
* [jalammar blog on Transformers](https://jalammar.github.io/illustrated-transformer/)
* [Harvard NLP group implementation notebook](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
* [Colab visualization of attention by tensorflow](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)
* [Blog on attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
* [jalammar blog on attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
